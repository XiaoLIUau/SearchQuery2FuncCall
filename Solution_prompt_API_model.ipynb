{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onIjldswiQu4"
      },
      "outputs": [],
      "source": [
        "! rm -r /content/SearchQuery2FuncCall\n",
        "!git clone https://github.com/XiaoLIUau/SearchQuery2FuncCall.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7pzF9QXiQu_"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade pip\n",
        "%pip install --disable-pip-version-check \\\n",
        "    torch==1.13.1 \\\n",
        "    torchdata==0.5.1\n",
        "\n",
        "%pip install --upgrade accelerate\\\n",
        "\n",
        "%pip install \\\n",
        "    transformers==4.28.1 \\\n",
        "    datasets==2.11.0 \\\n",
        "    evaluate==0.4.0 \\\n",
        "    rouge_score==0.1.2 \\\n",
        "    google.generativeai \\\n",
        "    langchain \\\n",
        "    cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJdMxKLyiQvA"
      },
      "outputs": [],
      "source": [
        "from time import sleep,time\n",
        "from getpass import getpass\n",
        "import pandas as pd\n",
        "import evaluate\n",
        "from langchain import PromptTemplate, LLMChain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and process dataset"
      ],
      "metadata": {
        "id": "a_rQ3x34iTPz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apwwt5-kiQvB"
      },
      "outputs": [],
      "source": [
        "from SearchQuery2FuncCall.setup_dataset import text2json, load_n_process_data\n",
        "\n",
        "text2json('/content/SearchQuery2FuncCall/Dataset.txt')\n",
        "# q2f_datasets = load_n_process_data('/content/non_search_examples.json')\n",
        "q2f_datasets = load_n_process_data('/content/q2f_dataset.json')\n",
        "q2f_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define prompt template\n"
      ],
      "metadata": {
        "id": "OSKsz_C4iW-i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OlD5AlciQvC"
      },
      "outputs": [],
      "source": [
        "# define prompt format\n",
        "def prompt_template():\n",
        "    start_prompt = 'Input:'\n",
        "    end_prompt = ', Output:'\n",
        "    instruction = f\"\"\"Instruction: Given a search query, then route to different backend components based on the search intent.\n",
        "1. If the search is about unit conversion, return API function UnitConvert(SourceUnit, TargetUnit, SourceValue).\n",
        "2. If the search is about calculation, return API function Calculate(Equation).\n",
        "3. If the search is about other search intent, return API function Search().\n",
        "* For unit conversion: common unit conversion in length, mass, time, area, speed, temperature, volume should be covered. And it should be consistent for the same unit throughout. E.g. it should always be “foot”, it cannot be “feet” or “ft” in API calls.\n",
        "* For calculation: common operation such as +, -, *, /, pow, log, ln, exp, tan(h), sin(h), cos(h), factorial should be covered. And it should be consistent for the same operation throughout. E.g. it should always be “ * ”, it cannot be “x” or “X” in API calls.\n",
        "Handle input queries in different language styles. Cover common unit conversion and calculation operations.\n",
        "\n",
        "Examples:\n",
        "{start_prompt} “ft to cm” {end_prompt} “UnitConvert(SourceUnit:foot, TargetUnit:centimeter,\n",
        "SourceValue:1)”\n",
        "{start_prompt} “how many ounces in 5.8 kilograms” {end_prompt} “UnitConvert(SourceUnit:kilogram,\n",
        "TargetUnit:ounce, SourceValue:5.8)”\n",
        "{start_prompt} “two to the power of 10” {end_prompt} “Calculate(2^10)”\n",
        "{start_prompt} “2001-1989” {end_prompt} “Calculate(2001-1989)”\n",
        "{start_prompt} “what is chatgpt” {end_prompt} “Search()”\n",
        "{start_prompt} “primary year 1 maths calculation checklist” {end_prompt} “Search()”\n",
        "{start_prompt} “what are different length units” {end_prompt} “Search()”\n",
        "{start_prompt} “Natural logarithm of -3/18” {end_prompt} “Calculate(ln(-3/18))”\n",
        "\n",
        "\"\"\"\n",
        "    template = instruction + start_prompt + '“{input}”' + end_prompt + '\\n'\n",
        "    return template"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load proprietary model API key\n",
        "Here we use Cohere and Palm models\n",
        "\n",
        "Note: Please load a text file that contains your model api key to current folder\n",
        "\n",
        ">Name your file in either ***'api_key_cohere.txt'*** or ***'api_key_palm.txt'*** accordingly\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YGFw0TmIiciq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52-TLL6riQvD"
      },
      "outputs": [],
      "source": [
        "\"\"\" # Get model api key \"\"\"\n",
        "def load_api_key_from_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        api_key = file.read().strip()\n",
        "    return api_key\n",
        "\n",
        "\"\"\"# Cohere Model API\"\"\"\n",
        "def setup_model_cohere():\n",
        "  from langchain.llms import Cohere\n",
        "  # from langchain.embeddings import CohereEmbeddings\n",
        "  api_key_file = \"api_key_cohere.txt\"\n",
        "  COHERE_API_KEY = load_api_key_from_file(api_key_file)\n",
        "  llm = Cohere(cohere_api_key=COHERE_API_KEY, temperature=0.0000000001)\n",
        "  # embeddings = CohereEmbeddings(cohere_api_key=COHERE_API_KEY)\n",
        "  return llm\n",
        "\n",
        "\"\"\"# Palm API\"\"\"\n",
        "def setup_model_palm():\n",
        "  # from langchain.embeddings import GooglePalmEmbeddings\n",
        "\tfrom langchain.llms import GooglePalm\n",
        "\timport google.generativeai as palm\n",
        "\n",
        "  # configure palm\n",
        "\tapi_key_file = \"api_key_palm.txt\"\n",
        "\tPalm_API_KEY = load_api_key_from_file(api_key_file)\n",
        "\n",
        "\tllm = GooglePalm(google_api_key=Palm_API_KEY, temperature = 0.0)\n",
        "\t# embeddings =GooglePalmEmbeddings(google_api_key=Palm_API_KEY)\n",
        "\n",
        "\treturn llm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Searh Query function to generate desided output using LangChain"
      ],
      "metadata": {
        "id": "ip7aJHGWk0xG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def searchQuery2(input, llm):\n",
        "\n",
        "  template = prompt_template()\n",
        "\n",
        "  prompt = PromptTemplate(template=template, input_variables=[\"input\"])\n",
        "\n",
        "  llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "  OUTPUTS=llm_chain.predict(input=input)\n",
        "\n",
        "  return OUTPUTS"
      ],
      "metadata": {
        "id": "icduLLmGk46N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post processing generation output from LLM"
      ],
      "metadata": {
        "id": "z6nhgQrJk_Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extractOutputString(input_string):\n",
        "    input_string = \"\".join(input_string.split())\n",
        "    prefixes = ['“', '”', \"'\", '\"']\n",
        "    if input_string.startswith(tuple(prefixes)):\n",
        "        input_string = input_string[1:]\n",
        "    if input_string.endswith(tuple(prefixes)):\n",
        "        input_string = input_string[:-1]\n",
        "    return input_string"
      ],
      "metadata": {
        "id": "91axBQGTk_3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate outputs for given test dataset\n",
        "Here we only test for index 50 to 70 in the test dataset"
      ],
      "metadata": {
        "id": "p6K-fPogllOc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nkjQ_6-iQvE"
      },
      "outputs": [],
      "source": [
        "dash_line = '-'.join('' for x in range(100))\n",
        "\n",
        "llm = setup_model_palm()\n",
        "# llm = setup_model_cohere()\n",
        "\n",
        "index_s=78\n",
        "index_e=index_s+len(q2f_datasets['test'])\n",
        "inputs = q2f_datasets['test'][index_s:index_e]['input']\n",
        "outputs = q2f_datasets['test'][index_s:index_e]['output']\n",
        "# inputs = q2f_datasets['test']['input']\n",
        "# outputs = q2f_datasets['test']['output']\n",
        "\n",
        "API_outputs = []\n",
        "\n",
        "for idx, input in enumerate(inputs):\n",
        "\n",
        "    API_output = searchQuery2(input, llm)\n",
        "    API_output = extractOutputString(API_output)\n",
        "    API_outputs.append(API_output)\n",
        "\n",
        "\n",
        "zipped_summaries = list(zip(inputs, outputs, API_outputs))\n",
        "\n",
        "df = pd.DataFrame(zipped_summaries, columns = ['inputs', 'outputs', 'API_outputs'])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate using ROUGE and BLEU scores\n"
      ],
      "metadata": {
        "id": "bmUh6zJwl3AQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnkpBcINiQvF"
      },
      "outputs": [],
      "source": [
        "# Rouge\n",
        "rouge = evaluate.load('rouge')\n",
        "API_model_results = rouge.compute(\n",
        "    predictions=API_outputs,\n",
        "    references=outputs[0:len(API_outputs)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "print('API MODEL ROUGE SCORES:')\n",
        "print(API_model_results)\n",
        "\n",
        "# bleu\n",
        "bleu = evaluate.load('bleu')\n",
        "API_model_results = bleu.compute(\n",
        "    predictions=API_outputs,\n",
        "    references=outputs,\n",
        ")\n",
        "\n",
        "print('API MODEL BLEU SCORES:')\n",
        "print(API_model_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW5BeursiQvG"
      },
      "source": [
        "## Results\n",
        "\n",
        "* Text examples for index 50:70\n",
        "\n",
        "### Cohere\n",
        "\n",
        "API MODEL ROUGE SCORES:\n",
        "{'rouge1': 0.9642857142857142, 'rouge2': 0.6060606060606061, 'rougeL': 0.9642857142857142, 'rougeLsum': 0.9642857142857142}\n",
        "\n",
        "API MODEL BLEU SCORES:\n",
        "{'bleu': 0.6720708576427871, 'precisions': [0.75, 0.7083333333333334, 0.6554054054054054, 0.5859375], 'brevity_penalty': 1.0, 'length_ratio': 1.3146853146853146, 'translation_length': 188, 'reference_length': 143}\n",
        "\n",
        "### Palm\n",
        "\n",
        "API MODEL ROUGE SCORES:\n",
        "{'rouge1': 0.9854545454545456, 'rouge2': 0.7849999999999999, 'rougeL': 0.9854545454545456, 'rougeLsum': 0.9854545454545456}\n",
        "\n",
        "API MODEL BLEU SCORES:\n",
        "{'bleu': 0.9159590736349433, 'precisions': [0.9623655913978495, 0.9337349397590361, 0.8972602739726028, 0.873015873015873], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 186, 'reference_length': 186}\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
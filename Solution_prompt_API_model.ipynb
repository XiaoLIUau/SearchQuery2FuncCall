{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r /content/SearchQuery2FuncCall\n",
    "!git clone https://github.com/XiaoLIUau/SearchQuery2FuncCall.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1\n",
    "\n",
    "%pip install --upgrade accelerate\\\n",
    "\n",
    "%pip install \\\n",
    "    transformers==4.28.1 \\\n",
    "    datasets==2.11.0 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    google.generativeai \\\n",
    "    langchain \\\n",
    "    cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep,time\n",
    "from getpass import getpass\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SearchQuery2FuncCall.setup_dataset import text2json, load_n_process_data\n",
    "\n",
    "text2json('/content/SearchQuery2FuncCall/Dataset.txt')\n",
    "# q2f_datasets = load_n_process_data('/content/non_search_examples.json')\n",
    "q2f_datasets = load_n_process_data('/content/q2f_dataset.json')\n",
    "q2f_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompt format\n",
    "def prompt_template():\n",
    "    start_prompt = '<User Query>:'\n",
    "    end_prompt = ', <API Call>: '\n",
    "    instruction = f\"\"\"Instruction: Given a search query, then route to different backend components based on the search intent.\n",
    "1. If the search is about unit conversion, return API function UnitConvert(SourceUnit, TargetUnit, SourceValue).\n",
    "2. If the search is about calculation, return API function Calculate(Equation).\n",
    "3. If the search is about other search intent, return API function Search(). \n",
    "Handle input queries in different language styles. Cover common unit conversion and calculation operations.\n",
    "\n",
    "Examples:\n",
    "{start_prompt}: “ft to cm”; {end_prompt}: “UnitConvert(SourceUnit:foot, TargetUnit:centimeter,\n",
    "SourceValue:1)”\n",
    "{start_prompt}: “how many ounces in 5.8 kilograms”; {end_prompt}: “UnitConvert(SourceUnit:kilogram,\n",
    "TargetUnit:ounce, SourceValue:5.8)”\n",
    "{start_prompt}: “two to the power of 10”, {end_prompt}: “Calculate(2^10)”\n",
    "{start_prompt}: “2001-1989”, {end_prompt}: “Calculate(2001-1989)”\n",
    "{start_prompt}: “what is chatgpt”, {end_prompt}: “Search()”\n",
    "{start_prompt}: “primary year 1 maths calculation checklist”, {end_prompt}: “Search()”\n",
    "{start_prompt}: “what are different length units”, {end_prompt}: “Search()”\n",
    "{start_prompt}: “Natural logarithm of -3/18“, {end_prompt}: “Calculate(ln(-3/18))”\n",
    "\n",
    "\"\"\"\n",
    "    template = instruction + start_prompt + '{input},' + end_prompt\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Get model api key \"\"\"\n",
    "def load_api_key_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        api_key = file.read().strip()\n",
    "    return api_key\n",
    "\n",
    "\"\"\"# Cohere Model API\"\"\"\n",
    "def setup_model_cohere():\n",
    "  from langchain.llms import Cohere\n",
    "  # from langchain.embeddings import CohereEmbeddings\n",
    "  api_key_file = \"api_key_cohere.txt\"\n",
    "  COHERE_API_KEY = load_api_key_from_file(api_key_file)\n",
    "  llm = Cohere(cohere_api_key=COHERE_API_KEY, temperature=0.0000000001)\n",
    "  # embeddings = CohereEmbeddings(cohere_api_key=COHERE_API_KEY)\n",
    "  return llm\n",
    "\n",
    "\"\"\"# Palm API\"\"\"\n",
    "def setup_model_palm():\n",
    "  # from langchain.embeddings import GooglePalmEmbeddings\n",
    "\tfrom langchain.llms import GooglePalm\n",
    "\timport google.generativeai as palm\n",
    "\n",
    "  # configure palm\n",
    "\tapi_key_file = \"api_key_palm.txt\"\n",
    "\tPalm_API_KEY = load_api_key_from_file(api_key_file)\n",
    "\n",
    "\tllm = GooglePalm(google_api_key=Palm_API_KEY, temperature = 0.0)\n",
    "\t# embeddings =GooglePalmEmbeddings(google_api_key=Palm_API_KEY)\n",
    "\n",
    "\treturn llm\n",
    "\n",
    "def searchQuery2(input, llm):\n",
    "  \n",
    "  template = prompt_template()\n",
    "\n",
    "  prompt = PromptTemplate(template=template, input_variables=[\"input\"])\n",
    "\n",
    "  llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "  OUTPUTS=llm_chain.predict(input=input)\n",
    "\n",
    "  return OUTPUTS\n",
    "\n",
    "def extractOutputString(input_string):\n",
    "    # if input_string.startswith(' '):\n",
    "    #     input_string = input_string[1:]\n",
    "    input_string = \"\".join(input_string.split())\n",
    "    if input_string.startswith('\"'):\n",
    "        input_string = input_string[1:]\n",
    "    if input_string.endswith('\"'):\n",
    "        input_string = input_string[:-1]\n",
    "    return input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "# llm = setup_model_palm()\n",
    "llm = setup_model_cohere()\n",
    "\n",
    "inputs = q2f_datasets['test'][50:70]['input']\n",
    "outputs = q2f_datasets['test'][50:70]['output']\n",
    "# inputs = q2f_datasets['test']['input']\n",
    "# outputs = q2f_datasets['test']['output']\n",
    "\n",
    "API_outputs = []\n",
    "\n",
    "for idx, input in enumerate(inputs):\n",
    "\n",
    "    API_output = searchQuery2(input, llm)\n",
    "    API_output = extractOutputString(API_output)\n",
    "    API_outputs.append(API_output)\n",
    "\n",
    "\n",
    "zipped_summaries = list(zip(inputs, outputs, API_outputs))\n",
    "\n",
    "df = pd.DataFrame(zipped_summaries, columns = ['inputs', 'outputs', 'API_outputs'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "API_model_results = rouge.compute(\n",
    "    predictions=API_outputs,\n",
    "    references=outputs[0:len(API_outputs)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('API MODEL ROUGE SCORES:')\n",
    "print(API_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "API_model_results = bleu.compute(\n",
    "    predictions=API_outputs,\n",
    "    references=outputs,\n",
    ")\n",
    "\n",
    "print('API MODEL BLEU SCORES:')\n",
    "print(API_model_results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

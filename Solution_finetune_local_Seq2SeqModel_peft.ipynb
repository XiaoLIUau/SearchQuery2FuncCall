{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r /content/SearchQuery2FuncCall\n",
    "!git clone https://github.com/XiaoLIUau/SearchQuery2FuncCall.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1\n",
    "\n",
    "%pip install --upgrade accelerate\\\n",
    "             --upgrade huggingface_hub\n",
    "\n",
    "%pip install \\\n",
    "    bitsandbytes>=0.39.0 \\\n",
    "    transformers==4.28.1 \\\n",
    "    datasets==2.11.0 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    loralib==0.1.1 \\\n",
    "    peft==0.3.0 --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig, GenerationConfig, TrainingArguments, Trainer\n",
    "\n",
    "from peft import PeftModel, LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SearchQuery2FuncCall.setup_dataset import text2json, load_n_process_data\n",
    "\n",
    "text2json('/content/SearchQuery2FuncCall/Dataset.txt')\n",
    "# q2f_datasets = load_n_process_data('/content/non_search_examples.json')\n",
    "q2f_datasets = load_n_process_data('/content/q2f_dataset.json')\n",
    "q2f_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Get model api key \"\"\"\n",
    "def load_api_key_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        api_key = file.read().strip()\n",
    "    return api_key\n",
    "\n",
    "# Setting a new environment variable\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = load_api_key_from_file('/content/api_key_huggingface.txt')\n",
    "\n",
    "!huggingface-cli login --token $HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_use_double_quant=True,\n",
    "    bnb_8bit_quant_type=\"nf4\",\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "    # llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "# Load model directly\n",
    "model_name = 'google/flan-t5-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.truncation_side = 'left'\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                  model_name,\n",
    "                  torch_dtype=torch.bfloat16, # need to check is this would work to reduce runtime memory\n",
    "                  device_map='auto',\n",
    "                  quantization_config=bnb_config,\n",
    "                  )\n",
    "\n",
    "# PEFT Setup\n",
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompt format\n",
    "def create_prompt(input):\n",
    "    start_prompt = 'Input:'#'<User Query>:'#\n",
    "    end_prompt = ', Output:'#', <API Call>: '#\n",
    "    instruction = f\"\"\"Instruction: Given a search query, then route to different backend components based on the search intent.\n",
    "1. If the search is about unit conversion, return API function UnitConvert(SourceUnit, TargetUnit, SourceValue).\n",
    "2. If the search is about calculation, return API function Calculate(Equation).\n",
    "3. If the search is about other search intent, return API function Search(). \n",
    "* For unit conversion: common unit conversion in length, mass, time, area, speed, temperature, volume should be covered. And it should be consistent for the same unit throughout. E.g. it should always be “foot”, it cannot be “feet” or “ft” in API calls.\n",
    "* For calculation: common operation such as +, -, *, /, pow, log, ln, exp, tan(h), sin(h), cos(h), factorial should be covered. And it should be consistent for the same operation throughout. E.g. it should always be “ * ”, it cannot be “x” or “X” in API calls.\n",
    "Handle input queries in different language styles. Cover common unit conversion and calculation operations.\n",
    "\n",
    "Examples:\n",
    "{start_prompt} “ft to cm” {end_prompt} “UnitConvert(SourceUnit:foot, TargetUnit:centimeter,\n",
    "SourceValue:1)”\n",
    "{start_prompt} “how many ounces in 5.8 kilograms” {end_prompt} “UnitConvert(SourceUnit:kilogram,\n",
    "TargetUnit:ounce, SourceValue:5.8)”\n",
    "{start_prompt} “two to the power of 10” {end_prompt} “Calculate(2^10)”\n",
    "{start_prompt} “2001-1989” {end_prompt} “Calculate(2001-1989)”\n",
    "{start_prompt} “what is chatgpt” {end_prompt} “Search()”\n",
    "{start_prompt} “primary year 1 maths calculation checklist” {end_prompt} “Search()”\n",
    "{start_prompt} “what are different length units” {end_prompt} “Search()”\n",
    "{start_prompt} “Natural logarithm of -3/18” {end_prompt} “Calculate(ln(-3/18))”\n",
    "\n",
    "\"\"\"\n",
    "    prompt = instruction + start_prompt + '“{input}”' + end_prompt + '\\n'\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_generate(original_model, inputs):\n",
    "    generation_config = original_model.generation_config\n",
    "    generation_config.max_new_tokens = 100\n",
    "    generation_config.temperature = 0.00000000000001\n",
    "    generation_config.top_p = 0.7\n",
    "    generation_config.num_return_sequences = 1\n",
    "    generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "    generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "    return original_model.generate(\n",
    "            input_ids = inputs.input_ids,\n",
    "            attention_mask = inputs.attention_mask,\n",
    "            generation_config = generation_config,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    prompt = [create_prompt(input) for input in example[\"input\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"output\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = q2f_datasets.shuffle().map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['input', 'output'])\n",
    "# tokenized_datasets = tokenized_datasets.remove_columns(['Search', 'input', 'output'])\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(original_model,\n",
    "                            lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./peft-query-function-training-{str(int(time.time()))}'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=40,\n",
    "    logging_steps=30,\n",
    "    max_steps=500\n",
    ")\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_trainer.train()\n",
    "\n",
    "peft_model_path=\"./peft-query-function-checkpoint-local\"\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = PeftModel.from_pretrained(original_model,\n",
    "                                       peft_model_path,\n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_path, torch_dtype=torch.bfloat16)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractOutputString(input_string):\n",
    "    input_string = \"\".join(input_string.split())\n",
    "    prefixes = ['“', '”', \"'\", '\"']\n",
    "    if input_string.startswith(tuple(prefixes)):\n",
    "        input_string = input_string[1:]\n",
    "    if input_string.endswith(tuple(prefixes)):\n",
    "        input_string = input_string[:-1]\n",
    "    return input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_indices = [9, 40, 50]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    input = q2f_datasets['test'][index]['input']\n",
    "    output = q2f_datasets['test'][index]['output']\n",
    "\n",
    "    prompt = create_prompt(input)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    generated = tokenizer.decode(\n",
    "        model_generate(peft_model, inputs)[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT:\\n{input}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE OUTPUT:\\n{output}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - OUTPUT:\\n{generated}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = q2f_datasets['test'][50:70]['input']\n",
    "outputs = q2f_datasets['test'][50:70]['output']\n",
    "# inputs = q2f_datasets['test']['input']\n",
    "# outputs = q2f_datasets['test']['output']\n",
    "\n",
    "API_outputs = []\n",
    "\n",
    "for idx, input in enumerate(inputs):\n",
    "\n",
    "    API_output = tokenizer.decode(\n",
    "        model_generate(peft_model, inputs)[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    API_output = extractOutputString(API_output)\n",
    "    API_outputs.append(API_output)\n",
    "\n",
    "\n",
    "zipped_summaries = list(zip(inputs, outputs, API_outputs))\n",
    "\n",
    "df = pd.DataFrame(zipped_summaries, columns = ['inputs', 'outputs', 'API_outputs'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "API_model_results = rouge.compute(\n",
    "    predictions=API_outputs,\n",
    "    references=outputs[0:len(API_outputs)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('API MODEL ROUGE SCORES:')\n",
    "print(API_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "API_model_results = bleu.compute(\n",
    "    predictions=API_outputs,\n",
    "    references=outputs,\n",
    ")\n",
    "\n",
    "print('API MODEL BLEU SCORES:')\n",
    "print(API_model_results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r /content/SearchQuery2FuncCall\n",
    "!git clone https://github.com/XiaoLIUau/SearchQuery2FuncCall.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1\n",
    "\n",
    "%pip install --upgrade accelerate\\\n",
    "             --upgrade huggingface_hub\n",
    "\n",
    "%pip install \\\n",
    "    bitsandbytes>=0.39.0 \\\n",
    "    transformers==4.28.1 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    loralib==0.1.1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SearchQuery2FuncCall.setup_dataset import text2json, load_n_process_data\n",
    "\n",
    "text2json('/content/SearchQuery2FuncCall/Dataset.txt')\n",
    "# q2f_datasets = load_n_process_data('/content/non_search_examples.json')\n",
    "q2f_datasets = load_n_process_data('/content/q2f_dataset.json')\n",
    "q2f_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from huggingface_hub import login\n",
    "# login()\n",
    "\n",
    "\"\"\" # Get model api key \"\"\"\n",
    "def load_api_key_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        api_key = file.read().strip()\n",
    "    return api_key\n",
    "\n",
    "# Setting a new environment variable\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = load_api_key_from_file('/content/api_key_huggingface.txt')\n",
    "\n",
    "!huggingface-cli login --token $HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_use_double_quant=True,\n",
    "    bnb_8bit_quant_type=\"nf4\",\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "    # llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "model_name=\"atwine/llama-2-7b-chat-fully-quantized-q4-06092023\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             device_map=\"auto\",\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             cache_dir=\"/tmp/model_cache/\",\n",
    "                                             offload_folder=\"/path/to/offload_folder\"\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompt format\n",
    "def create_prompt(input):\n",
    "    start_prompt = 'Input:'#'<User Query>:'#\n",
    "    end_prompt = ', Output:'#', <API Call>: '#\n",
    "    instruction = f\"\"\"Instruction: Given a search query, then route to different backend components based on the search intent.\n",
    "1. If the search is about unit conversion, return API function UnitConvert(SourceUnit, TargetUnit, SourceValue).\n",
    "2. If the search is about calculation, return API function Calculate(Equation).\n",
    "3. If the search is about other search intent, return API function Search(). \n",
    "* For unit conversion: common unit conversion in length, mass, time, area, speed, temperature, volume should be covered. And it should be consistent for the same unit throughout. E.g. it should always be “foot”, it cannot be “feet” or “ft” in API calls.\n",
    "* For calculation: common operation such as +, -, *, /, pow, log, ln, exp, tan(h), sin(h), cos(h), factorial should be covered. And it should be consistent for the same operation throughout. E.g. it should always be “ * ”, it cannot be “x” or “X” in API calls.\n",
    "Handle input queries in different language styles. Cover common unit conversion and calculation operations.\n",
    "\n",
    "Examples:\n",
    "{start_prompt}“ft to cm”{end_prompt}“UnitConvert(SourceUnit:foot, TargetUnit:centimeter,\n",
    "SourceValue:1)”\n",
    "{start_prompt}“how many ounces in 5.8 kilograms”{end_prompt}“UnitConvert(SourceUnit:kilogram,\n",
    "TargetUnit:ounce, SourceValue:5.8)”\n",
    "{start_prompt}“two to the power of 10”{end_prompt}“Calculate(2^10)”\n",
    "{start_prompt}“2001-1989” {end_prompt}“Calculate(2001-1989)”\n",
    "{start_prompt}“what is chatgpt”{end_prompt}“Search()”\n",
    "{start_prompt}“primary year 1 maths calculation checklist”{end_prompt}“Search()”\n",
    "{start_prompt}“what are different length units”{end_prompt}“Search()”\n",
    "{start_prompt}“Natural logarithm of -3/18”{end_prompt}“Calculate(ln(-3/18))”\n",
    "\n",
    "Only return output of the the given input\n",
    "\"\"\"\n",
    "    prompt = instruction + start_prompt + f'“{input}”' + end_prompt\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_generate(original_model, inputs):\n",
    "    generation_config = original_model.generation_config\n",
    "    generation_config.max_new_tokens = 50\n",
    "    generation_config.temperature = 0.00000000000001\n",
    "    generation_config.top_p = 0.7\n",
    "    generation_config.num_return_sequences = 1\n",
    "    generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "    generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "    return original_model.generate(\n",
    "            input_ids = inputs.input_ids,\n",
    "            attention_mask = inputs.attention_mask,\n",
    "            generation_config = generation_config,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractOutputString(input_string):\n",
    "    input_string = \"\".join(input_string.split())\n",
    "    prefixes = ['“', '”', \"'\", '\"']\n",
    "    if input_string.startswith(tuple(prefixes)):\n",
    "        input_string = input_string[1:]\n",
    "    if input_string.endswith(tuple(prefixes)):\n",
    "        input_string = input_string[:-1]\n",
    "    return input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_indices = [9, 40, 50]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    input = q2f_datasets['test'][index]['input']\n",
    "    output = q2f_datasets['test'][index]['output']\n",
    "\n",
    "    prompt = create_prompt(input)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    generated = tokenizer.decode(\n",
    "        model_generate(original_model, inputs)[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT:\\n{input}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE OUTPUT:\\n{output}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - OUTPUT:\\n{generated}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_s=50\n",
    "index_e=index_s+20\n",
    "inputs = q2f_datasets['test'][index_s:index_e]['input']\n",
    "outputs = q2f_datasets['test'][index_s:index_e]['output']\n",
    "# inputs = q2f_datasets['test']['input']\n",
    "# outputs = q2f_datasets['test']['output']\n",
    "\n",
    "API_outputs = []\n",
    "\n",
    "for idx, input in enumerate(inputs):\n",
    "\n",
    "    API_output = model_generate(input)\n",
    "    API_output = extractOutputString(API_output)\n",
    "    API_outputs.append(API_output)\n",
    "\n",
    "\n",
    "zipped_summaries = list(zip(inputs, outputs, API_outputs))\n",
    "\n",
    "df = pd.DataFrame(zipped_summaries, columns = ['inputs', 'outputs', 'API_outputs'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "API_model_results = rouge.compute(\n",
    "    predictions=API_outputs,\n",
    "    references=outputs[0:len(API_outputs)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('API MODEL ROUGE SCORES:')\n",
    "print(API_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "API_model_results = bleu.compute(\n",
    "    predictions=API_outputs,\n",
    "    references=outputs,\n",
    ")\n",
    "\n",
    "print('API MODEL BLEU SCORES:')\n",
    "print(API_model_results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

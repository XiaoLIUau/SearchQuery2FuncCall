{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBNHAhID8K7U"
      },
      "outputs": [],
      "source": [
        "! rm -r /content/SearchQuery2FuncCall\n",
        "!git clone https://github.com/XiaoLIUau/SearchQuery2FuncCall.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZ8S_qbp8K7a"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade pip\n",
        "%pip install --disable-pip-version-check \\\n",
        "    torch==1.13.1 \\\n",
        "    torchdata==0.5.1\n",
        "\n",
        "%pip install --upgrade accelerate\\\n",
        "             --upgrade huggingface_hub\n",
        "\n",
        "%pip install \\\n",
        "    bitsandbytes>=0.39.0 \\\n",
        "    transformers==4.28.1 \\\n",
        "    evaluate==0.4.0 \\\n",
        "    rouge_score==0.1.2 \\\n",
        "    loralib==0.1.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYmhm_NJ8K7b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import evaluate\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and process dataset"
      ],
      "metadata": {
        "id": "vYVrWK7VepoQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONqh-af_8K7c"
      },
      "outputs": [],
      "source": [
        "from SearchQuery2FuncCall.setup_dataset import text2json, load_n_process_data\n",
        "\n",
        "text2json('/content/SearchQuery2FuncCall/Dataset.txt')\n",
        "# q2f_datasets = load_n_process_data('/content/non_search_examples.json')\n",
        "q2f_datasets = load_n_process_data('/content/q2f_dataset.json')\n",
        "q2f_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Huggingface API key\n",
        "Here we use Huggingface models\n",
        "\n",
        "Note: Please load a text file that contains your model api key to current folder\n",
        "\n",
        ">Name your file in either ***'api_key_huggingface.txt'***\n"
      ],
      "metadata": {
        "id": "vlSTNHNWeq7h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkmO9ZCa8K7d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from huggingface_hub import login\n",
        "# login()\n",
        "\n",
        "\"\"\" # Get model api key \"\"\"\n",
        "def load_api_key_from_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        api_key = file.read().strip()\n",
        "    return api_key\n",
        "\n",
        "# Setting a new environment variable\n",
        "os.environ[\"HUGGINGFACE_TOKEN\"] = load_api_key_from_file('/content/api_key_huggingface.txt')\n",
        "\n",
        "!huggingface-cli login --token $HUGGINGFACE_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load model"
      ],
      "metadata": {
        "id": "IYd6z9k4e-i3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqnMpdPN8K7e"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_use_double_quant=True,\n",
        "    bnb_8bit_quant_type=\"nf4\",\n",
        "    bnb_8bit_compute_dtype=torch.bfloat16,\n",
        "    # llm_int8_enable_fp32_cpu_offload=True\n",
        ")\n",
        "model_name=\"atwine/llama-2-7b-chat-fully-quantized-q4-06092023\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
        "original_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\",\n",
        "                                             quantization_config=bnb_config,\n",
        "                                             torch_dtype=torch.bfloat16,\n",
        "                                             cache_dir=\"/tmp/model_cache/\",\n",
        "                                             offload_folder=\"/path/to/offload_folder\"\n",
        "                                             )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Print model trainable parameters"
      ],
      "metadata": {
        "id": "xYKqvmgVfFAV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNFot73X8K7f"
      },
      "outputs": [],
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
        "\n",
        "print(print_number_of_trainable_model_parameters(original_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define prompt template"
      ],
      "metadata": {
        "id": "Bv3Fv-FNfKKP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYPBFJ8E8K7f"
      },
      "outputs": [],
      "source": [
        "# define prompt format\n",
        "def create_prompt(input):\n",
        "    start_prompt = 'Input:'#'<User Query>:'#\n",
        "    end_prompt = ', Output:'#', <API Call>: '#\n",
        "    instruction = f\"\"\"Instruction: Given a search query, then route to different backend components based on the search intent.\n",
        "1. If the search is about unit conversion, return API function UnitConvert(SourceUnit, TargetUnit, SourceValue).\n",
        "2. If the search is about calculation, return API function Calculate(Equation).\n",
        "3. If the search is about other search intent, return API function Search().\n",
        "* For unit conversion: common unit conversion in length, mass, time, area, speed, temperature, volume should be covered. And it should be consistent for the same unit throughout. E.g. it should always be “foot”, it cannot be “feet” or “ft” in API calls.\n",
        "* For calculation: common operation such as +, -, *, /, pow, log, ln, exp, tan(h), sin(h), cos(h), factorial should be covered. And it should be consistent for the same operation throughout. E.g. it should always be “ * ”, it cannot be “x” or “X” in API calls.\n",
        "Handle input queries in different language styles. Cover common unit conversion and calculation operations.\n",
        "\n",
        "Examples:\n",
        "{start_prompt}“ft to cm”{end_prompt}“UnitConvert(SourceUnit:foot, TargetUnit:centimeter,\n",
        "SourceValue:1)”\n",
        "{start_prompt}“how many ounces in 5.8 kilograms”{end_prompt}“UnitConvert(SourceUnit:kilogram,\n",
        "TargetUnit:ounce, SourceValue:5.8)”\n",
        "{start_prompt}“two to the power of 10”{end_prompt}“Calculate(2^10)”\n",
        "{start_prompt}“2001-1989” {end_prompt}“Calculate(2001-1989)”\n",
        "{start_prompt}“what is chatgpt”{end_prompt}“Search()”\n",
        "{start_prompt}“primary year 1 maths calculation checklist”{end_prompt}“Search()”\n",
        "{start_prompt}“what are different length units”{end_prompt}“Search()”\n",
        "{start_prompt}“Natural logarithm of -3/18”{end_prompt}“Calculate(ln(-3/18))”\n",
        "\n",
        "Only return output of the the given input.\n",
        "\n",
        "\"\"\"\n",
        "    prompt = instruction + start_prompt + f'“{input}”' + end_prompt\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate response in tokens from loaded model\n",
        "The input_ids and generations are tokens"
      ],
      "metadata": {
        "id": "oNgjZOs7fPEz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tK0K9aiq8K7h"
      },
      "outputs": [],
      "source": [
        "def model_generate(original_model, inputs):\n",
        "    generation_config = original_model.generation_config\n",
        "    generation_config.max_new_tokens = 30\n",
        "    generation_config.temperature = 0.00000000000001\n",
        "    generation_config.top_p = 0.9\n",
        "    generation_config.num_return_sequences = 1\n",
        "    generation_config.pad_token_id = tokenizer.eos_token_id\n",
        "    generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "    return original_model.generate(\n",
        "            input_ids = inputs.input_ids,\n",
        "            attention_mask = inputs.attention_mask,\n",
        "            generation_config = generation_config,\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate respsons in text using giving input query text"
      ],
      "metadata": {
        "id": "Ec84H_Hdff2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generated_text(input):\n",
        "    prompt = create_prompt(input)\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    generated = tokenizer.decode(\n",
        "        model_generate(original_model, inputs)[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    return generated"
      ],
      "metadata": {
        "id": "oWY3hY1GEiTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post processing generation output from LLM\n"
      ],
      "metadata": {
        "id": "1NxBs4_7f0pK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxDr6ffR8K7i"
      },
      "outputs": [],
      "source": [
        "def extractOutputString(input_string,output_string):\n",
        "    import re\n",
        "    # Use regular expressions to find the matching output for the input query\n",
        "    output_match = re.search(rf'Input:\\s*“{re.escape(input_string)}”\\s*,\\s*Output:\\s*“([^\"]+)”\\s*', output_string, flags=re.MULTILINE)\n",
        "    # Extract and print the output\n",
        "    if output_match:\n",
        "        output_string = output_match.group(1)\n",
        "    # Remove quotation marks\n",
        "    prefixes = ['“', '”', \"'\", '\"']\n",
        "    if output_string.startswith(tuple(prefixes)):\n",
        "        output_string = output_string[1:]\n",
        "    if output_string.endswith(tuple(prefixes)):\n",
        "        output_string = output_string[:-1]\n",
        "    # Remove all space in output\n",
        "    output_string = \"\".join(output_string.split())\n",
        "    return output_string"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate with selected examples"
      ],
      "metadata": {
        "id": "wVzR6atBf7-b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2gmlhtg8K7j"
      },
      "outputs": [],
      "source": [
        "example_indices = [9, 40, 50]\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "\n",
        "for index in example_indices:\n",
        "    input = q2f_datasets['test'][index]['input']\n",
        "    output = q2f_datasets['test'][index]['output']\n",
        "\n",
        "    generated = generated_text(input)\n",
        "    generated = extractOutputString(input,generated)\n",
        "\n",
        "    print(dash_line)\n",
        "    print('Example ', i + 1)\n",
        "    print(dash_line)\n",
        "    print(f'INPUT:\\n{input}')\n",
        "    print(dash_line)\n",
        "    print(f'BASELINE OUTPUT:\\n{output}')\n",
        "    print(dash_line)\n",
        "    print(f'MODEL GENERATION - OUTPUT:\\n{generated}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate outputs for given test dataset\n",
        "Here we are using the test dataset"
      ],
      "metadata": {
        "id": "M-TTuCdCgTc4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-M3wRBv8K7k"
      },
      "outputs": [],
      "source": [
        "index_s=0\n",
        "index_e=index_s+len(q2f_datasets['test'])\n",
        "inputs = q2f_datasets['test'][index_s:index_e]['input']\n",
        "outputs = q2f_datasets['test'][index_s:index_e]['output']\n",
        "# inputs = q2f_datasets['test']['input']\n",
        "# outputs = q2f_datasets['test']['output']\n",
        "\n",
        "API_outputs = []\n",
        "\n",
        "for idx, input in enumerate(inputs):\n",
        "    API_output = generated_text(input)\n",
        "    API_output = extractOutputString(input,API_output)\n",
        "    API_outputs.append(API_output)\n",
        "\n",
        "\n",
        "zipped_summaries = list(zip(inputs, outputs, API_outputs))\n",
        "\n",
        "df = pd.DataFrame(zipped_summaries, columns = ['inputs', 'outputs', 'API_outputs'])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate using ROUGE and BLEU scores"
      ],
      "metadata": {
        "id": "hfF4ryJAgkQs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3mYq3tn8K7l"
      },
      "outputs": [],
      "source": [
        "# Rouge\n",
        "rouge = evaluate.load('rouge')\n",
        "API_model_results = rouge.compute(\n",
        "    predictions=API_outputs,\n",
        "    references=outputs[0:len(API_outputs)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "print('API MODEL ROUGE SCORES:')\n",
        "print(API_model_results)\n",
        "\n",
        "# bleu\n",
        "bleu = evaluate.load('bleu')\n",
        "API_model_results = bleu.compute(\n",
        "    predictions=API_outputs,\n",
        "    references=outputs,\n",
        ")\n",
        "\n",
        "print('API MODEL BLEU SCORES:')\n",
        "print(API_model_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "* Text examples for index 50:70\n",
        "\n",
        "\n",
        "### API MODEL ROUGE SCORES:\n",
        "\n",
        ">{'rouge1': 0.9371428571428572, 'rouge2': 0.5419047619047619, 'rougeL': 0.9399999999999998, 'rougeLsum': 0.9371428571428572}\n",
        "\n",
        "### API MODEL BLEU SCORES:\n",
        "\n",
        ">{'bleu': 0.867918734298719, 'precisions': [0.9074074074074074, 0.8802816901408451, 0.8524590163934426, 0.8333333333333334], 'brevity_penalty': 1.0, 'length_ratio': 1.0657894736842106, 'translation_length': 162, 'reference_length': 152}"
      ],
      "metadata": {
        "id": "rQ4yohw7dJSI"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
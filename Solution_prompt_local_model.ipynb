{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r /content/SearchQuery2FuncCall\n",
    "!git clone https://github.com/XiaoLIUau/SearchQuery2FuncCall.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1\n",
    "\n",
    "%pip install --upgrade accelerate\\\n",
    "             --upgrade huggingface_hub\n",
    "\n",
    "%pip install \\\n",
    "    bitsandbytes>=0.39.0 \\\n",
    "    transformers==4.28.1 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    loralib==0.1.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SearchQuery2FuncCall.setup_dataset import text2json, load_n_process_data\n",
    "\n",
    "text2json('/content/SearchQuery2FuncCall/Dataset.txt')\n",
    "# q2f_datasets = load_n_process_data('/content/non_search_examples.json')\n",
    "q2f_datasets = load_n_process_data('/content/q2f_dataset.json')\n",
    "q2f_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# login()\n",
    "\n",
    "\"\"\" # Get model api key \"\"\"\n",
    "def load_api_key_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        api_key = file.read().strip()\n",
    "    return api_key\n",
    "\n",
    "# Setting a new environment variable\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = load_api_key_from_file('/content/api_key_huggingface.txt')\n",
    "\n",
    "!huggingface-cli login --token $HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_use_double_quant=True,\n",
    "    bnb_8bit_quant_type=\"nf4\",\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "    # llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "model_name=\"atwine/llama-2-7b-chat-fully-quantized-q4-06092023\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             device_map=\"auto\",\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             cache_dir=\"/tmp/model_cache/\",\n",
    "                                             offload_folder=\"/path/to/offload_folder\"\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompt format\n",
    "def create_prompt(input):\n",
    "    start_prompt = 'Input:'#'<User Query>:'#\n",
    "    end_prompt = ', Output:'#', <API Call>: '#\n",
    "    instruction = f\"\"\"Instruction: Given a search query, then route to different backend components based on the search intent.\n",
    "1. If the search is about unit conversion, return API function UnitConvert(SourceUnit, TargetUnit, SourceValue).\n",
    "2. If the search is about calculation, return API function Calculate(Equation).\n",
    "3. If the search is about other search intent, return API function Search(). \n",
    "* For unit conversion: common unit conversion in length, mass, time, area, speed, temperature, volume should be covered. And it should be consistent for the same unit throughout. E.g. it should always be “foot”, it cannot be “feet” or “ft” in API calls.\n",
    "* For calculation: common operation such as +, -, *, /, pow, log, ln, exp, tan(h), sin(h), cos(h), factorial should be covered. And it should be consistent for the same operation throughout. E.g. it should always be “ * ”, it cannot be “x” or “X” in API calls.\n",
    "Handle input queries in different language styles. Cover common unit conversion and calculation operations.\n",
    "\n",
    "Examples:\n",
    "{start_prompt}“ft to cm”{end_prompt}“UnitConvert(SourceUnit:foot, TargetUnit:centimeter,\n",
    "SourceValue:1)”\n",
    "{start_prompt}“how many ounces in 5.8 kilograms”{end_prompt}“UnitConvert(SourceUnit:kilogram,\n",
    "TargetUnit:ounce, SourceValue:5.8)”\n",
    "{start_prompt}“two to the power of 10”{end_prompt}“Calculate(2^10)”\n",
    "{start_prompt}“2001-1989” {end_prompt}“Calculate(2001-1989)”\n",
    "{start_prompt}“what is chatgpt”{end_prompt}“Search()”\n",
    "{start_prompt}“primary year 1 maths calculation checklist”{end_prompt}“Search()”\n",
    "{start_prompt}“what are different length units”{end_prompt}“Search()”\n",
    "{start_prompt}“Natural logarithm of -3/18”{end_prompt}“Calculate(ln(-3/18))”\n",
    "\n",
    "Only return output of the the given input.\n",
    "\n",
    "\"\"\"\n",
    "    prompt = instruction + start_prompt + f'“{input}”' + end_prompt\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_generate(original_model, inputs):\n",
    "    generation_config = original_model.generation_config\n",
    "    generation_config.max_new_tokens = 30\n",
    "    generation_config.temperature = 0.00000000000001\n",
    "    generation_config.top_p = 0.9\n",
    "    generation_config.num_return_sequences = 1\n",
    "    generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "    generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "    return original_model.generate(\n",
    "            input_ids = inputs.input_ids,\n",
    "            attention_mask = inputs.attention_mask,\n",
    "            generation_config = generation_config,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generated_text(input):\n",
    "    prompt = create_prompt(input)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    generated = tokenizer.decode(\n",
    "        model_generate(original_model, inputs)[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractOutputString(input_string,output_string):\n",
    "    import re\n",
    "    # Use regular expressions to find the matching output for the input query\n",
    "    output_match = re.search(rf'Input:\\s*“{re.escape(input_string)}”\\s*,\\s*Output:\\s*“([^\"]+)”\\s*', output_string, flags=re.MULTILINE)\n",
    "    # Extract and print the output\n",
    "    if output_match:\n",
    "        output_string = output_match.group(1)\n",
    "    # Remove quotation marks\n",
    "    prefixes = ['“', '”', \"'\", '\"']\n",
    "    if output_string.startswith(tuple(prefixes)):\n",
    "        output_string = output_string[1:]\n",
    "    if output_string.endswith(tuple(prefixes)):\n",
    "        output_string = output_string[:-1]\n",
    "    # Remove all space in output\n",
    "    output_string = \"\".join(output_string.split())\n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_indices = [9, 40, 50]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "for index in example_indices:\n",
    "    input = q2f_datasets['test'][index]['input']\n",
    "    output = q2f_datasets['test'][index]['output']\n",
    "\n",
    "    generated = generated_text(input)\n",
    "    generated = extractOutputString(input,generated)\n",
    "\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT:\\n{input}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE OUTPUT:\\n{output}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - OUTPUT:\\n{generated}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_s=50\n",
    "index_e=index_s+20\n",
    "inputs = q2f_datasets['test'][index_s:index_e]['input']\n",
    "outputs = q2f_datasets['test'][index_s:index_e]['output']\n",
    "# inputs = q2f_datasets['test']['input']\n",
    "# outputs = q2f_datasets['test']['output']\n",
    "\n",
    "API_outputs = []\n",
    "\n",
    "for idx, input in enumerate(inputs):\n",
    "    API_output = generated_text(input)\n",
    "    API_output = extractOutputString(input,API_output)\n",
    "    API_outputs.append(API_output)\n",
    "\n",
    "\n",
    "zipped_summaries = list(zip(inputs, outputs, API_outputs))\n",
    "\n",
    "df = pd.DataFrame(zipped_summaries, columns = ['inputs', 'outputs', 'API_outputs'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "API_model_results = rouge.compute(\n",
    "    predictions=API_outputs,\n",
    "    references=outputs[0:len(API_outputs)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('API MODEL ROUGE SCORES:')\n",
    "print(API_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "* Text examples for index 50:70\n",
    "\n",
    "\n",
    "### API MODEL ROUGE SCORES:\n",
    "\n",
    "{'rouge1': 0.9371428571428572, 'rouge2': 0.5419047619047619, 'rougeL': 0.9399999999999998, 'rougeLsum': 0.9371428571428572}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "API_model_results = bleu.compute(\n",
    "    predictions=API_outputs,\n",
    "    references=outputs,\n",
    ")\n",
    "\n",
    "print('API MODEL BLEU SCORES:')\n",
    "print(API_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "* Text examples for index 50:70\n",
    "\n",
    "\n",
    "### API MODEL BLEU SCORES:\n",
    "\n",
    "{'bleu': 0.867918734298719, 'precisions': [0.9074074074074074, 0.8802816901408451, 0.8524590163934426, 0.8333333333333334], 'brevity_penalty': 1.0, 'length_ratio': 1.0657894736842106, 'translation_length': 162, 'reference_length': 152}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

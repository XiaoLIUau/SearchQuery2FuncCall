{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpCLZIwGuow1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sk-Grjf1vC6D"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade pip\n",
        "%pip install --disable-pip-version-check \\\n",
        "    torch==1.13.1 \\\n",
        "    torchdata==0.5.1 --quiet\n",
        "\n",
        "%pip install \\\n",
        "    transformers==4.27.2 \\\n",
        "    datasets==2.11.0 \\\n",
        "    evaluate==0.4.0 \\\n",
        "    rouge_score==0.1.2 \\\n",
        "    loralib==0.1.1 \\\n",
        "    peft==0.3.0 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEP4ZNujvFYo"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
        "import torch\n",
        "import time\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDK2tvft019F"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset('json', data_files='flan_t5_q2f_dataset.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1W6soz8g4rYl"
      },
      "outputs": [],
      "source": [
        "q2f_datasets=dataset.shuffle(seed=42)\n",
        "q2f_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ajuIDqL4tJ7"
      },
      "outputs": [],
      "source": [
        "datasets_train_test = q2f_datasets[\"train\"].train_test_split(test_size=80)\n",
        "datasets_train_validation = datasets_train_test[\"train\"].train_test_split(test_size=50)\n",
        "\n",
        "q2f_datasets[\"train\"] = datasets_train_validation[\"train\"]\n",
        "q2f_datasets[\"validation\"] = datasets_train_validation[\"test\"]\n",
        "q2f_datasets[\"test\"] = datasets_train_test[\"test\"]\n",
        "q2f_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwrY0Cx15Stq"
      },
      "outputs": [],
      "source": [
        "example_indices = [9, 40, 50]\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "\n",
        "for i, index in enumerate(example_indices):\n",
        "    print(dash_line)\n",
        "    print('Example ', i + 1)\n",
        "    print(dash_line)\n",
        "    print('INPUT:')\n",
        "    print(q2f_datasets['test'][index]['input'])\n",
        "    print(dash_line)\n",
        "    print('OUTPUT:')\n",
        "    print(q2f_datasets['test'][index]['output'])\n",
        "    print(dash_line)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSvYEDXn55dV"
      },
      "outputs": [],
      "source": [
        "model_name='google/flan-t5-base'\n",
        "\n",
        "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEol7fNt6NkV"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbGuIkh3MOc3"
      },
      "outputs": [],
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
        "\n",
        "print(print_number_of_trainable_model_parameters(original_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6inMaLs6vCw"
      },
      "outputs": [],
      "source": [
        "sentence = \"miles to kilometers\"\n",
        "\n",
        "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
        "\n",
        "sentence_decoded = tokenizer.decode(\n",
        "        sentence_encoded[\"input_ids\"][0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "print('ENCODED SENTENCE:')\n",
        "print(sentence_encoded[\"input_ids\"][0])\n",
        "print('\\nDECODED SENTENCE:')\n",
        "print(sentence_decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3asTqgX66ab"
      },
      "outputs": [],
      "source": [
        "for i, index in enumerate(example_indices):\n",
        "    input = q2f_datasets['test'][index]['input']\n",
        "    output = q2f_datasets['test'][index]['output']\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Input:\n",
        "\n",
        "{input}\n",
        "\n",
        "Output: ?\n",
        "    \"\"\"\n",
        "\n",
        "    # Input constructed prompt instead of the dialogue.\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    generated = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    print(dash_line)\n",
        "    print('Example ', i + 1)\n",
        "    print(dash_line)\n",
        "    print(f'INPUT:\\n{input}')\n",
        "    print(dash_line)\n",
        "    print(f'BASELINE OUTPUT:\\n{output}')\n",
        "    print(dash_line)\n",
        "    print(f'MODEL GENERATION - OUTPUT:\\n{generated}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u_0_KmmR4rJ"
      },
      "source": [
        "# Few-shot-prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mBMQSzI7quH"
      },
      "outputs": [],
      "source": [
        "def make_prompt(example_indices_full, example_index_to_translate):\n",
        "    prompt = ''\n",
        "    for index in example_indices_full:\n",
        "        input = q2f_datasets['test'][index]['input']\n",
        "        output = q2f_datasets['test'][index]['output']\n",
        "\n",
        "        # The stop sequence '{ouptut}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n",
        "        prompt += f\"\"\"\n",
        "Input:\n",
        "\n",
        "{input}\n",
        "\n",
        "Output:\n",
        "{output}\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    input = q2f_datasets['test'][example_index_to_translate]['input']\n",
        "\n",
        "    prompt += f\"\"\"\n",
        "Input:\n",
        "\n",
        "{input}\n",
        "\n",
        "Output:\n",
        "\"\"\"\n",
        "\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEB2xW9F90Uv"
      },
      "outputs": [],
      "source": [
        "example_indices_full = [9, 40, 50]\n",
        "example_index_to_translate = 60\n",
        "\n",
        "few_shot_prompt = make_prompt(example_indices_full, example_index_to_translate)\n",
        "\n",
        "print(few_shot_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sMBQhYZ92eN"
      },
      "outputs": [],
      "source": [
        "output = q2f_datasets['test'][example_index_to_translate]['output']\n",
        "\n",
        "\n",
        "generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.00000000000001)\n",
        "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
        "translate = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        generation_config=generation_config,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(dash_line)\n",
        "print(f'BASELINE OUTPUT:\\n{output}\\n')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - FEW SHOTS:\\n{translate}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvyBEsBfOLW1"
      },
      "source": [
        "# Preprocess Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PllSx3c3OKBR"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(example):\n",
        "    start_prompt = 'Input:\\n\\n'\n",
        "    end_prompt = '\\n\\nOutput: '\n",
        "    prompt = [start_prompt + input + end_prompt for input in example[\"input\"]]\n",
        "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "    example['labels'] = tokenizer(example[\"output\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    return example\n",
        "\n",
        "# The dataset actually contains 3 diff splits: train, validation, test.\n",
        "# The tokenize_function code is handling all data across all splits in batches.\n",
        "tokenized_datasets = q2f_datasets.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns(['input', 'output',])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhSmB1CLOwGk"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAIwh82dBeCb"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32, # Rank\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
        ")\n",
        "peft_model = get_peft_model(original_model,\n",
        "                            lora_config)\n",
        "print(print_number_of_trainable_model_parameters(peft_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-Jd8wfOPduy"
      },
      "source": [
        "<a name='3.2'></a>\n",
        "### 3.2 - Train PEFT Adapter\n",
        "\n",
        "Define training arguments and create `Trainer` instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1lXH5bmNUxH"
      },
      "outputs": [],
      "source": [
        "output_dir = f'./peft-query-fucntion-training-{str(int(time.time()))}'\n",
        "\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3,\n",
        "    num_train_epochs=5,\n",
        "    logging_steps=1,\n",
        "    max_steps=1\n",
        ")\n",
        "\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=peft_training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSSK2DAvP_hc"
      },
      "outputs": [],
      "source": [
        "peft_trainer.train()\n",
        "\n",
        "peft_model_path=\"./peft-query-fucntion-checkpoint-local\"\n",
        "\n",
        "peft_trainer.model.save_pretrained(peft_model_path)\n",
        "tokenizer.save_pretrained(peft_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTqUcmm6QVgP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
